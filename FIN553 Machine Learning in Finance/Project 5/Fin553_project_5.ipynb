{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KS4HH5iwM0Je"
   },
   "source": [
    "Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFAzfmE5-5uY",
    "outputId": "adc9e52f-a7a6-4b57-efae-57b99395d0b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Value Iteration Results ===\n",
      "Value iteration converged after 20 iterations\n",
      "\n",
      "=== Policy Iteration Results ===\n",
      "Policy iteration converged after 2 iterations\n",
      "\n",
      "Value Function Comparison:\n",
      "State               Value Iteration    Policy Iteration\n",
      "-------------------------------------------------------\n",
      "Phase 0              $   3.321M        $   3.321M\n",
      "Phase 1 (Promising)  $   6.745M        $   6.745M\n",
      "Phase 1 (Disappointing) $   0.585M        $   0.585M\n",
      "Success              $  10.000M        $  10.000M\n",
      "Failure              $   0.000M        $   0.000M\n",
      "\n",
      "Optimal Policy Comparison:\n",
      "State               Value Iteration    Policy Iteration\n",
      "-------------------------------------------------------\n",
      "Phase 0              Invest $100k     Invest $100k\n",
      "Phase 1 (Promising)  Invest $100k     Invest $100k\n",
      "Phase 1 (Disappointing) No investment    No investment\n",
      "Success              Terminal state   Terminal state\n",
      "Failure              Terminal state   Terminal state\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MDP():\n",
    "    def __init__(self):\n",
    "        self.A = [0, 1]\n",
    "        self.S = [0, 1, 2, 3, 4]\n",
    "        self.gamma = 0.996\n",
    "\n",
    "        # Transition probabilities for no investment (action 0)\n",
    "        P0 = np.array([[0.5, .15, .15, 0, .20],\n",
    "                      [0, .5, .0, .25, .25],\n",
    "                      [0, 0, .15, .05, .8],\n",
    "                      [0, 0, 0, 0, 1],\n",
    "                      [0, 0, 0, 0, 1]])\n",
    "\n",
    "        R0 = np.array([0, 0, 0, 10, 0])\n",
    "\n",
    "        # Transition probabilities for investment (action 1)\n",
    "        P1 = np.array([[0.5, .25, .15, 0, .10],\n",
    "                      [0, .5, .0, .35, .15],\n",
    "                      [0, 0, .20, .05, .75],\n",
    "                      [0, 0, 0, 0, 1],\n",
    "                      [0, 0, 0, 0, 1]])\n",
    "\n",
    "        R1 = np.array([-0.1, -0.1, -0.1, 10, 0])\n",
    "\n",
    "        self.P = [P0, P1]\n",
    "        self.R = [R0, R1]\n",
    "\n",
    "    def step(self, s, a):\n",
    "        s_prime = np.random.choice(len(self.S), p=self.P[a][s])\n",
    "        R = self.R[a][s]\n",
    "        if s_prime == 4:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        return s_prime, R, done\n",
    "\n",
    "    def simulate(self, s, a, π):\n",
    "        done = False\n",
    "        t = 0\n",
    "        history = []\n",
    "        while not done:\n",
    "            if t > 0:\n",
    "                a = π[s]\n",
    "            s_prime, R, done = self.step(s, a)\n",
    "            history.append((s, a, R))\n",
    "            s = s_prime\n",
    "            t += 1\n",
    "        return history\n",
    "\n",
    "    def value_iteration(self):\n",
    "        V = np.zeros(len(self.S))\n",
    "        V[3] = 10  # Success state value\n",
    "\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            iteration += 1\n",
    "            delta = 0\n",
    "            V_old = V.copy()\n",
    "\n",
    "            for s in range(3):  # Non-terminal states\n",
    "                action_values = []\n",
    "                for a in self.A:\n",
    "                    immediate_reward = self.R[a][s]\n",
    "                    future_value = 0\n",
    "                    for s_prime in range(len(self.S)):\n",
    "                        prob = self.P[a][s][s_prime]\n",
    "                        future_value += prob * V_old[s_prime]\n",
    "                    action_values.append(immediate_reward + self.gamma * future_value)\n",
    "\n",
    "                V[s] = max(action_values)\n",
    "                delta = max(delta, abs(V[s] - V_old[s]))\n",
    "\n",
    "            if delta < 0.0001:\n",
    "                print(f\"Value iteration converged after {iteration} iterations\")\n",
    "                break\n",
    "\n",
    "        # Get policy from value function\n",
    "        policy = np.zeros(len(self.S))\n",
    "        for s in range(3):  # Non-terminal states\n",
    "            action_values = []\n",
    "            for a in self.A:\n",
    "                immediate_reward = self.R[a][s]\n",
    "                future_value = 0\n",
    "                for s_prime in range(len(self.S)):\n",
    "                    prob = self.P[a][s][s_prime]\n",
    "                    future_value += prob * V[s_prime]\n",
    "                action_values.append(immediate_reward + self.gamma * future_value)\n",
    "            policy[s] = np.argmax(action_values)\n",
    "\n",
    "        policy[3] = -1  # Terminal states\n",
    "        policy[4] = -1\n",
    "\n",
    "        return V, policy\n",
    "\n",
    "    def policy_iteration(self):\n",
    "        V = np.zeros(len(self.S))\n",
    "        V[3] = 10  # Success state value\n",
    "        policy = np.zeros(len(self.S), dtype=int)\n",
    "\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            iteration += 1\n",
    "            # Policy Evaluation\n",
    "            while True:\n",
    "                delta = 0\n",
    "                for s in range(3):\n",
    "                    v = V[s]\n",
    "                    a = policy[s]\n",
    "\n",
    "                    immediate_reward = self.R[a][s]\n",
    "                    future_value = 0\n",
    "                    for s_prime in range(len(self.S)):\n",
    "                        prob = self.P[a][s][s_prime]\n",
    "                        future_value += prob * V[s_prime]\n",
    "\n",
    "                    V[s] = immediate_reward + self.gamma * future_value\n",
    "                    delta = max(delta, abs(v - V[s]))\n",
    "\n",
    "                if delta < 0.0001:\n",
    "                    break\n",
    "\n",
    "            # Policy Improvement\n",
    "            policy_stable = True\n",
    "            for s in range(3):\n",
    "                old_action = policy[s]\n",
    "\n",
    "                action_values = []\n",
    "                for a in self.A:\n",
    "                    immediate_reward = self.R[a][s]\n",
    "                    future_value = 0\n",
    "                    for s_prime in range(len(self.S)):\n",
    "                        prob = self.P[a][s][s_prime]\n",
    "                        future_value += prob * V[s_prime]\n",
    "                    action_values.append(immediate_reward + self.gamma * future_value)\n",
    "\n",
    "                policy[s] = np.argmax(action_values)\n",
    "\n",
    "                if old_action != policy[s]:\n",
    "                    policy_stable = False\n",
    "\n",
    "            if policy_stable:\n",
    "                print(f\"Policy iteration converged after {iteration} iterations\")\n",
    "                break\n",
    "\n",
    "        policy[3] = -1  # Terminal states\n",
    "        policy[4] = -1\n",
    "\n",
    "        return V, policy\n",
    "\n",
    "# Create MDP instance and solve using both methods\n",
    "mdp = MDP()\n",
    "\n",
    "# Solve using Value Iteration\n",
    "print(\"\\n=== Value Iteration Results ===\")\n",
    "V_value, policy_value = mdp.value_iteration()\n",
    "\n",
    "# Solve using Policy Iteration\n",
    "print(\"\\n=== Policy Iteration Results ===\")\n",
    "V_policy, policy_policy = mdp.policy_iteration()\n",
    "\n",
    "# Print results for both methods\n",
    "state_names = [\"Phase 0\", \"Phase 1 (Promising)\", \"Phase 1 (Disappointing)\", \"Success\", \"Failure\"]\n",
    "\n",
    "print(\"\\nValue Function Comparison:\")\n",
    "print(\"State               Value Iteration    Policy Iteration\")\n",
    "print(\"-\" * 55)\n",
    "for s in range(len(mdp.S)):\n",
    "    print(f\"{state_names[s]:<20} ${V_value[s]:>8.3f}M        ${V_policy[s]:>8.3f}M\")\n",
    "\n",
    "print(\"\\nOptimal Policy Comparison:\")\n",
    "print(\"State               Value Iteration    Policy Iteration\")\n",
    "print(\"-\" * 55)\n",
    "for s in range(len(mdp.S)):\n",
    "    if policy_value[s] == -1:\n",
    "        action_value = \"Terminal state\"\n",
    "        action_policy = \"Terminal state\"\n",
    "    else:\n",
    "        action_value = \"Invest $100k\" if policy_value[s] == 1 else \"No investment\"\n",
    "        action_policy = \"Invest $100k\" if policy_policy[s] == 1 else \"No investment\"\n",
    "    print(f\"{state_names[s]:<20} {action_value:<16} {action_policy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SP9_nXhhMzRM"
   },
   "source": [
    "Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EnL7Ckl-7PQe",
    "outputId": "bd5a9c62-4ba0-494c-d3b4-444e74fad1aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Value Iteration Results ===\n",
      "Value iteration converged after 20 iterations\n",
      "\n",
      "=== Policy Iteration Results ===\n",
      "Policy iteration converged after 2 iterations\n",
      "\n",
      "Value Function Comparison:\n",
      "State               Value Iteration    Policy Iteration\n",
      "-------------------------------------------------------\n",
      "Phase 0              $   3.321M        $   3.321M\n",
      "Phase 1 (Promising)  $   6.745M        $   6.745M\n",
      "Phase 1 (Disappointing) $   0.585M        $   0.585M\n",
      "Success              $  10.000M        $  10.000M\n",
      "Failure              $   0.000M        $   0.000M\n",
      "\n",
      "Optimal Policy Comparison:\n",
      "State               Value Iteration    Policy Iteration\n",
      "-------------------------------------------------------\n",
      "Phase 0              Invest $100k     Invest $100k\n",
      "Phase 1 (Promising)  Invest $100k     Invest $100k\n",
      "Phase 1 (Disappointing) No investment    No investment\n",
      "Success              Terminal state   Terminal state\n",
      "Failure              Terminal state   Terminal state\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class VaccineMDP():\n",
    "    def __init__(self):\n",
    "        # Initialize MDP parameters\n",
    "        self.A = [0, 1]  # Actions: 0 = no investment, 1 = invest $100k\n",
    "        self.S = [0, 1, 2, 3, 4]  # States\n",
    "        self.gamma = 0.996  # Discount factor\n",
    "\n",
    "        # Transition probabilities for no investment (action 0)\n",
    "        self.P0 = np.array([\n",
    "            [0.5, 0.15, 0.15, 0, 0.20],\n",
    "            [0, 0.5, 0.0, 0.25, 0.25],\n",
    "            [0, 0, 0.15, 0.05, 0.8],\n",
    "            [0, 0, 0, 1, 0],  # Success state stays in success\n",
    "            [0, 0, 0, 0, 1]   # Failure state stays in failure\n",
    "        ])\n",
    "\n",
    "        # Transition probabilities for investment (action 1)\n",
    "        self.P1 = np.array([\n",
    "            [0.5, 0.25, 0.15, 0, 0.10],\n",
    "            [0, 0.5, 0.0, 0.35, 0.15],\n",
    "            [0, 0, 0.20, 0.05, 0.75],\n",
    "            [0, 0, 0, 1, 0],  # Success state stays in success\n",
    "            [0, 0, 0, 0, 1]   # Failure state stays in failure\n",
    "        ])\n",
    "\n",
    "        # Rewards for each action\n",
    "        self.R0 = np.array([0, 0, 0, 10, 0])  # No investment\n",
    "        self.R1 = np.array([-0.1, -0.1, -0.1, 10, 0])  # Investment ($100k = 0.1M)\n",
    "\n",
    "        self.P = [self.P0, self.P1]\n",
    "        self.R = [self.R0, self.R1]\n",
    "\n",
    "    def value_iteration(self, theta=0.0001, max_iterations=1000):\n",
    "        # Initialize value function with terminal state values\n",
    "        V = np.zeros(len(self.S))\n",
    "        V[3] = 10  # Success state value = $10M\n",
    "        V[4] = 0   # Failure state value = $0M\n",
    "\n",
    "        for i in range(max_iterations):\n",
    "            delta = 0\n",
    "            V_old = V.copy()\n",
    "\n",
    "            # For each non-terminal state\n",
    "            for s in range(3):  # Only iterate through states 0, 1, and 2\n",
    "                # Calculate value for each action\n",
    "                action_values = []\n",
    "                for a in self.A:\n",
    "                    immediate_reward = self.R[a][s]\n",
    "                    future_value = 0\n",
    "                    for s_prime in range(len(self.S)):\n",
    "                        prob = self.P[a][s][s_prime]\n",
    "                        future_value += prob * V_old[s_prime]\n",
    "                    action_values.append(immediate_reward + self.gamma * future_value)\n",
    "\n",
    "                V[s] = max(action_values)\n",
    "                delta = max(delta, abs(V[s] - V_old[s]))\n",
    "\n",
    "            if delta < theta:\n",
    "                print(f\"Value iteration converged after {i+1} iterations\")\n",
    "                break\n",
    "\n",
    "        return V\n",
    "\n",
    "    def policy_iteration(self, theta=0.0001, max_iterations=1000):\n",
    "        # Initialize random policy and value function\n",
    "        policy = np.zeros(len(self.S), dtype=int)\n",
    "        V = np.zeros(len(self.S))\n",
    "        V[3] = 10  # Success state value = $10M\n",
    "        V[4] = 0   # Failure state value = $0M\n",
    "\n",
    "        for i in range(max_iterations):\n",
    "            # Policy Evaluation\n",
    "            while True:\n",
    "                delta = 0\n",
    "                for s in range(3):  # Only non-terminal states\n",
    "                    v = V[s]\n",
    "                    a = policy[s]\n",
    "\n",
    "                    # Calculate new value\n",
    "                    immediate_reward = self.R[a][s]\n",
    "                    future_value = 0\n",
    "                    for s_prime in range(len(self.S)):\n",
    "                        prob = self.P[a][s][s_prime]\n",
    "                        future_value += prob * V[s_prime]\n",
    "\n",
    "                    V[s] = immediate_reward + self.gamma * future_value\n",
    "                    delta = max(delta, abs(v - V[s]))\n",
    "\n",
    "                if delta < theta:\n",
    "                    break\n",
    "\n",
    "            # Policy Improvement\n",
    "            policy_stable = True\n",
    "            for s in range(3):  # Only non-terminal states\n",
    "                old_action = policy[s]\n",
    "\n",
    "                # Find best action\n",
    "                action_values = []\n",
    "                for a in self.A:\n",
    "                    immediate_reward = self.R[a][s]\n",
    "                    future_value = 0\n",
    "                    for s_prime in range(len(self.S)):\n",
    "                        prob = self.P[a][s][s_prime]\n",
    "                        future_value += prob * V[s_prime]\n",
    "                    action_values.append(immediate_reward + self.gamma * future_value)\n",
    "\n",
    "                policy[s] = np.argmax(action_values)\n",
    "\n",
    "                if old_action != policy[s]:\n",
    "                    policy_stable = False\n",
    "\n",
    "            if policy_stable:\n",
    "                print(f\"Policy iteration converged after {i+1} iterations\")\n",
    "                break\n",
    "\n",
    "        # Set terminal state policies\n",
    "        policy[3] = -1  # Success state\n",
    "        policy[4] = -1  # Failure state\n",
    "\n",
    "        return V, policy\n",
    "\n",
    "    def get_optimal_policy(self, V):\n",
    "        policy = np.zeros(len(self.S))\n",
    "\n",
    "        # Only compute policy for non-terminal states\n",
    "        for s in range(3):\n",
    "            action_values = []\n",
    "            for a in self.A:\n",
    "                immediate_reward = self.R[a][s]\n",
    "                future_value = 0\n",
    "                for s_prime in range(len(self.S)):\n",
    "                    prob = self.P[a][s][s_prime]\n",
    "                    future_value += prob * V[s_prime]\n",
    "                action_values.append(immediate_reward + self.gamma * future_value)\n",
    "\n",
    "            policy[s] = np.argmax(action_values)\n",
    "\n",
    "        policy[3] = -1  # Success state\n",
    "        policy[4] = -1  # Failure state\n",
    "\n",
    "        return policy\n",
    "\n",
    "# Create MDP instance and solve using both methods\n",
    "mdp = VaccineMDP()\n",
    "\n",
    "# Solve using Value Iteration\n",
    "print(\"\\n=== Value Iteration Results ===\")\n",
    "V_value = mdp.value_iteration()\n",
    "policy_value = mdp.get_optimal_policy(V_value)\n",
    "\n",
    "# Solve using Policy Iteration\n",
    "print(\"\\n=== Policy Iteration Results ===\")\n",
    "V_policy, policy_policy = mdp.policy_iteration()\n",
    "\n",
    "# Print results for both methods\n",
    "state_names = [\"Phase 0\", \"Phase 1 (Promising)\", \"Phase 1 (Disappointing)\", \"Success\", \"Failure\"]\n",
    "\n",
    "print(\"\\nValue Function Comparison:\")\n",
    "print(\"State               Value Iteration    Policy Iteration\")\n",
    "print(\"-\" * 55)\n",
    "for s in range(len(mdp.S)):\n",
    "    print(f\"{state_names[s]:<20} ${V_value[s]:>8.3f}M        ${V_policy[s]:>8.3f}M\")\n",
    "\n",
    "print(\"\\nOptimal Policy Comparison:\")\n",
    "print(\"State               Value Iteration    Policy Iteration\")\n",
    "print(\"-\" * 55)\n",
    "for s in range(len(mdp.S)):\n",
    "    if policy_value[s] == -1:\n",
    "        action_value = \"Terminal state\"\n",
    "        action_policy = \"Terminal state\"\n",
    "    else:\n",
    "        action_value = \"Invest $100k\" if policy_value[s] == 1 else \"No investment\"\n",
    "        action_policy = \"Invest $100k\" if policy_policy[s] == 1 else \"No investment\"\n",
    "    print(f\"{state_names[s]:<20} {action_value:<16} {action_policy}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
