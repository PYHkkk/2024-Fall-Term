{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iT4heQUg8F3Y"
   },
   "source": [
    "Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0eqpYSlB5qa",
    "outputId": "9581828c-c83b-4966-ccfd-a34d08410e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating policies...\n",
      "\n",
      "Value function for Always Invest policy:\n",
      "State 0: $3.345M\n",
      "State 1: $6.729M\n",
      "State 2: $0.520M\n",
      "State 3: $10.000M\n",
      "State 4: $0.000M\n",
      "\n",
      "Value function for Never Invest policy:\n",
      "State 0: $1.666M\n",
      "State 1: $5.055M\n",
      "State 2: $0.596M\n",
      "State 3: $10.000M\n",
      "State 4: $0.000M\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "class MDP():\n",
    "    def __init__(self):\n",
    "        self.A = [0, 1]\n",
    "        self.S = [0, 1, 2, 3, 4]\n",
    "\n",
    "        P0 = np.array([[0.5, .15, .15, 0, .20],\n",
    "                       [0, .5, .0, .25, .25],\n",
    "                       [0, 0, .15, .05, .8],\n",
    "                       [0, 0, 0, 0, 1],\n",
    "                       [0, 0, 0, 0, 1]])\n",
    "\n",
    "        R0 = np.array([0, 0, 0, 10, 0])\n",
    "\n",
    "        P1 = np.array([[0.5, .25, .15, 0, .10],\n",
    "                       [0, .5, .0, .35, .15],\n",
    "                       [0, 0, .20, .05, .75],\n",
    "                       [0, 0, 0, 0, 1],\n",
    "                       [0, 0, 0, 0, 1]])\n",
    "\n",
    "        R1 = np.array([-0.1, -0.1, -0.1, 10, 0])\n",
    "\n",
    "        self.P = [P0, P1]\n",
    "        self.R = [R0, R1]\n",
    "\n",
    "    def step(self, s, a):\n",
    "        s_prime = np.random.choice(len(self.S), p=self.P[a][s])\n",
    "        R = self.R[a][s]\n",
    "        if s_prime == 4:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        return s_prime, R, done\n",
    "\n",
    "    def simulate(self, s, a, π):\n",
    "        done = False\n",
    "        t = 0\n",
    "        history = []\n",
    "        while not done:\n",
    "            if t > 0:\n",
    "                a = π[s]\n",
    "            s_prime, R, done = self.step(s, a)\n",
    "            history.append((s, a, R))\n",
    "            s = s_prime\n",
    "            t += 1\n",
    "        return history\n",
    "\n",
    "def monte_carlo_policy_evaluation(mdp, policy, num_episodes=10000, gamma=0.9960):\n",
    "    V = {s: 0 for s in range(5)}\n",
    "    N = {s: 0 for s in range(5)}\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        s = 0\n",
    "        a = policy[0]\n",
    "\n",
    "        history = mdp.simulate(s, a, policy)\n",
    "\n",
    "        G = 0\n",
    "        for t in range(len(history)-1, -1, -1):\n",
    "            s_t, _, r_t = history[t]\n",
    "            G = gamma * G + r_t\n",
    "\n",
    "            first_visit = True\n",
    "            for i in range(t):\n",
    "                if history[i][0] == s_t:\n",
    "                    first_visit = False\n",
    "                    break\n",
    "\n",
    "            if first_visit:\n",
    "                N[s_t] += 1\n",
    "                V[s_t] += (G - V[s_t]) / N[s_t]\n",
    "\n",
    "    return V\n",
    "\n",
    "# Create MDP instance\n",
    "mdp = MDP()\n",
    "\n",
    "# Define policies\n",
    "always_invest = [1, 1, 1, 1]  # Action 1 (invest) for all states\n",
    "never_invest = [0, 0, 0, 0]   # Action 0 (don't invest) for all states\n",
    "\n",
    "# Evaluate both policies\n",
    "print(\"Evaluating policies...\")\n",
    "V_always_invest = monte_carlo_policy_evaluation(mdp, always_invest)\n",
    "V_never_invest = monte_carlo_policy_evaluation(mdp, never_invest)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nValue function for Always Invest policy:\")\n",
    "for state, value in V_always_invest.items():\n",
    "    print(f\"State {state}: ${value:.3f}M\")\n",
    "\n",
    "print(\"\\nValue function for Never Invest policy:\")\n",
    "for state, value in V_never_invest.items():\n",
    "    print(f\"State {state}: ${value:.3f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kP-7MV_J8EWT"
   },
   "source": [
    "Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPtQ-n5T0at_",
    "outputId": "08591dbd-0b23-45ba-eabb-ef44ed628ec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating policies...\n",
      "\n",
      "Value function for Always Invest policy:\n",
      "State 0: $3.345M\n",
      "State 1: $6.729M\n",
      "State 2: $0.520M\n",
      "State 3: $10.000M\n",
      "State 4: $0.000M\n",
      "\n",
      "Value function for Never Invest policy:\n",
      "State 0: $1.666M\n",
      "State 1: $5.055M\n",
      "State 2: $0.596M\n",
      "State 3: $10.000M\n",
      "State 4: $0.000M\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MDP():\n",
    "    def __init__(self):\n",
    "        self.A = [0, 1]  # Actions: 0 = don't invest, 1 = invest\n",
    "        self.S = [0, 1, 2, 3, 4]  # States: phases of development\n",
    "\n",
    "        # Transition probabilities without investment\n",
    "        P0 = np.array([[0.5, .15, .15, 0, .20],\n",
    "                       [0, .5, .0, .25, .25],\n",
    "                       [0, 0, .15, .05, .8],\n",
    "                       [0, 0, 0, 0, 1],\n",
    "                       [0, 0, 0, 0, 1]])\n",
    "\n",
    "        # Rewards without investment\n",
    "        R0 = np.array([0, 0, 0, 10, 0])\n",
    "\n",
    "        # Transition probabilities with investment\n",
    "        P1 = np.array([[0.5, .25, .15, 0, .10],\n",
    "                       [0, .5, .0, .35, .15],\n",
    "                       [0, 0, .20, .05, .75],\n",
    "                       [0, 0, 0, 0, 1],\n",
    "                       [0, 0, 0, 0, 1]])\n",
    "\n",
    "        # Rewards with investment (-0.1 is the $100,000 investment cost)\n",
    "        R1 = np.array([-0.1, -0.1, -0.1, 10, 0])\n",
    "\n",
    "        self.P = [P0, P1]\n",
    "        self.R = [R0, R1]\n",
    "\n",
    "    def step(self, s, a):\n",
    "        s_prime = np.random.choice(len(self.S), p=self.P[a][s])\n",
    "        R = self.R[a][s]\n",
    "        if s_prime == 4:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        return s_prime, R, done\n",
    "\n",
    "    def simulate(self, s, a, π):\n",
    "        done = False\n",
    "        t = 0\n",
    "        history = []\n",
    "        while not done:\n",
    "            if t > 0:\n",
    "                a = π[s]\n",
    "            s_prime, R, done = self.step(s, a)\n",
    "            history.append((s, a, R))\n",
    "            s = s_prime\n",
    "            t += 1\n",
    "        return history\n",
    "\n",
    "def monte_carlo_policy_evaluation(mdp, policy, num_episodes=10000, gamma=0.9960):\n",
    "    # Initialize value function and visit counts\n",
    "    V = {s: 0 for s in range(5)}\n",
    "    N = {s: 0 for s in range(5)}\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Start from phase 0\n",
    "        s = 0\n",
    "        # Get first action according to policy\n",
    "        a = policy[0]\n",
    "\n",
    "        # Generate episode\n",
    "        history = mdp.simulate(s, a, policy)\n",
    "\n",
    "        # Calculate returns for each state in the episode\n",
    "        G = 0\n",
    "        for t in range(len(history)-1, -1, -1):\n",
    "            s_t, _, r_t = history[t]\n",
    "            G = gamma * G + r_t\n",
    "\n",
    "            # First-visit MC: only update if this is first visit to s_t in episode\n",
    "            first_visit = True\n",
    "            for i in range(t):\n",
    "                if history[i][0] == s_t:\n",
    "                    first_visit = False\n",
    "                    break\n",
    "\n",
    "            if first_visit:\n",
    "                N[s_t] += 1\n",
    "                V[s_t] += (G - V[s_t]) / N[s_t]\n",
    "\n",
    "    return V\n",
    "\n",
    "def main():\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Create MDP instance\n",
    "    mdp = MDP()\n",
    "\n",
    "    # Define policies\n",
    "    always_invest = [1, 1, 1, 1]  # Action 1 (invest) for all states\n",
    "    never_invest = [0, 0, 0, 0]   # Action 0 (don't invest) for all states\n",
    "\n",
    "    # Evaluate both policies\n",
    "    print(\"Evaluating policies...\")\n",
    "    V_always_invest = monte_carlo_policy_evaluation(mdp, always_invest)\n",
    "    V_never_invest = monte_carlo_policy_evaluation(mdp, never_invest)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nValue function for Always Invest policy:\")\n",
    "    for state, value in V_always_invest.items():\n",
    "        print(f\"State {state}: ${value:.3f}M\")\n",
    "\n",
    "    print(\"\\nValue function for Never Invest policy:\")\n",
    "    for state, value in V_never_invest.items():\n",
    "        print(f\"State {state}: ${value:.3f}M\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
